# Choosing an Embedding Model

CodeContext supports any HuggingFace embedding model compatible with the transformers library. This guide helps you choose the right model for your needs.

---

## Quick Comparison

| Model | Params | Dim | CoIR | License | Memory | Best For |
|-------|--------|-----|------|---------|--------|----------|
| **jina-code-0.5b** ðŸ¥‡ | 494M | 896 | **78.41** | CC-BY-NC-4.0 | ~1.9GB | Maximum search quality (non-commercial) |
| **jina-code-0.5b** | 600M | 1024 | N/A | Apache 2.0 | ~2.3GB | Multilingual, general-purpose |
| **Qodo-1.5B** | 1.5B | 1536 | 68.53 | OpenRAIL++-M | ~3GB | Commercial projects |
| **jina-v2-base-code** | 161M | ? | N/A | Apache 2.0 | ~800MB | Resource-constrained |
| **jina-code-1.5B** | 1.5B | 1536 | **79.04** | CC-BY-NC-4.0 | ~3GB | Maximum quality (non-commercial) |

---

## Decision Tree

```
Start here
â”‚
â”œâ”€ Commercial use required?
â”‚  â”œâ”€ Yes â†’ Qodo-Embed-1.5B (only commercial option)
â”‚  â””â”€ No â†’ Continue
â”‚
â”œâ”€ Resource constraints?
â”‚  â”œâ”€ <8GB RAM â†’ jina-v2-base-code (161M)
â”‚  â”œâ”€ <12GB RAM â†’ jina-code-0.5b (494M) âœ“ Recommended
â”‚  â””â”€ â‰¥16GB RAM â†’ Continue
â”‚
â”œâ”€ Priority?
â”‚  â”œâ”€ Maximum quality â†’ jina-code-1.5B (CoIR 79.04)
â”‚  â”œâ”€ Balanced â†’ jina-code-0.5b (CoIR 78.41) âœ“ Recommended
â”‚  â””â”€ Multilingual â†’ jina-code-0.5b
â”‚
â””â”€ Speed critical?
   â”œâ”€ Yes â†’ jina-v2-base-code or jina-code-0.5b
   â””â”€ No â†’ Any model based on quality needs
```

---

## Detailed Model Profiles

### ðŸ¥‡ jinaai/jina-code-embeddings-0.5b (Recommended)

**Best balance of quality and efficiency**

```yaml
embeddings:
  huggingface:
    model_name: "jinaai/jina-code-embeddings-0.5b"
    batch_size: 64
```

**Specs:**
- **Performance:** CoIR 78.41 (SOTA for <1B models)
- **Size:** 494M parameters, 896-dim embeddings
- **Memory:** ~1.9GB (FP16)
- **Speed:** Fast inference (smaller than competitors)
- **Languages:** 15+ (Python, Java, TS, JS, Go, C++, C#, PHP, Ruby, SQL, etc.)
- **Tasks:** nl2code, code2code, qa, code2nl, completion

**Pros:**
âœ… Excellent search quality (beats 1B+ models)
âœ… Efficient memory usage
âœ… Fast inference
âœ… Code-specialized training

**Cons:**
âŒ CC-BY-NC-4.0 license (non-commercial only)

**Best for:**
- Non-commercial projects prioritizing quality
- Medium-spec machines (8-12GB RAM)
- Code-focused search

---

### âš–ï¸ jinaai/jina-code-embeddings-0.5b

**General-purpose with multilingual support**

```yaml
embeddings:
  huggingface:
    model_name: "jinaai/jina-code-embeddings-0.5b"
    batch_size: 64
```

**Specs:**
- **Performance:** N/A (not benchmarked on CoIR)
- **Size:** 600M parameters, 1024-dim embeddings
- **Memory:** ~2.3GB (FP16)
- **Languages:** 100+ (multilingual focus)
- **Context:** 32k tokens

**Pros:**
âœ… Apache 2.0 license (permissive)
âœ… Multilingual (100+ languages)
âœ… Larger embedding dimension (1024)
âœ… General-purpose versatility

**Cons:**
âŒ Not code-specialized
âŒ Unknown CoIR performance

**Best for:**
- Multilingual codebases
- Mixed code + documentation projects
- License-sensitive environments (Apache 2.0)

---

### ðŸ¢ Qodo/Qodo-Embed-1-1.5B

**Only commercial-friendly code model**

```yaml
embeddings:
  huggingface:
    model_name: "Qodo/Qodo-Embed-1-1.5B"
    batch_size: 32  # Larger model requires smaller batch
```

**Specs:**
- **Performance:** CoIR 68.53
- **Size:** 1.5B parameters, 1536-dim embeddings
- **Memory:** ~3GB (FP16)
- **Languages:** 9 (Python, Java, TS, JS, Go, C++, C#, PHP, Ruby)
- **License:** OpenRAIL++-M (commercial allowed)

**Pros:**
âœ… OpenRAIL++-M license (commercial use)
âœ… Code-specialized
âœ… Qwen2-1.5B based (proven architecture)

**Cons:**
âŒ Lower CoIR score (68.53 vs 78.41)
âŒ 3x larger than jina-code-0.5b
âŒ Higher memory requirements
âŒ Slower inference

**Best for:**
- Commercial/enterprise projects
- License compliance critical
- Sufficient resources (â‰¥16GB RAM)

---

### ðŸª¶ jinaai/jina-embeddings-v2-base-code

**Lightweight for resource-constrained environments**

```yaml
embeddings:
  huggingface:
    model_name: "jinaai/jina-embeddings-v2-base-code"
    batch_size: 128  # Small model can use large batches
```

**Specs:**
- **Performance:** Unknown CoIR (older model)
- **Size:** 161M parameters
- **Memory:** ~800MB (FP16)
- **Languages:** 30+ programming languages
- **License:** Apache 2.0

**Pros:**
âœ… Very small (161M)
âœ… Low memory footprint
âœ… Fast inference
âœ… Apache 2.0 license

**Cons:**
âŒ Older architecture (v2, 2024.02)
âŒ Unknown CoIR score
âŒ May have lower quality than newer models

**Best for:**
- <8GB RAM systems
- CPU-only environments
- Speed-critical applications
- Prototyping/testing

---

### ðŸš€ jinaai/jina-code-embeddings-1.5B

**Maximum quality (non-commercial)**

```yaml
embeddings:
  huggingface:
    model_name: "jinaai/jina-code-embeddings-1.5b"
    batch_size: 32
```

**Specs:**
- **Performance:** CoIR 79.04 (SOTA)
- **Size:** 1.5B parameters, 1536-dim embeddings
- **Memory:** ~3GB (FP16)
- **Languages:** 15+ programming languages

**Pros:**
âœ… Highest CoIR score (79.04)
âœ… Maximum search quality
âœ… Code-specialized

**Cons:**
âŒ CC-BY-NC-4.0 (non-commercial only)
âŒ Large memory requirements
âŒ Slower inference than 0.5B

**Best for:**
- Research/academic projects
- Maximum quality priority
- Sufficient resources (â‰¥16GB RAM)

---

## Performance Comparison

### Search Quality (CoIR Benchmark)

```
jina-code-1.5B:    79.04 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (SOTA)
jina-code-0.5B:    78.41 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ (Recommended)
Qodo-1.5B:         68.53 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
jina-v2-base-code: N/A   (older, not benchmarked)
jina-code-0.5b:        N/A   (general-purpose, not benchmarked)
```

### Indexing Speed (10k files, M2 Mac, GPU)

```
jina-v2-base-code: 6-8 min   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
jina-code-0.5B:    8-10 min  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
jina-code-0.5b:        10-12 min â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
Qodo-1.5B:         12-15 min â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
jina-code-1.5B:    12-15 min â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
```

### Memory Usage (FP16)

```
jina-v2-base-code: ~0.8GB â–ˆâ–ˆâ–ˆâ–ˆ
jina-code-0.5B:    ~1.9GB â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ
jina-code-0.5b:        ~2.3GB â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ
Qodo-1.5B:         ~3.0GB â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
jina-code-1.5B:    ~3.0GB â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
```

---

## Switching Models

### Simple Switch

1. Edit `.codecontext.yaml`:
   ```yaml
   embeddings:
     huggingface:
       model_name: "your-new-model"
       batch_size: [adjust-based-on-model-size]
   ```

2. Re-index:
   ```bash
   codecontext index
   ```

**Important:** Different models have different embedding dimensions. Re-indexing is required when switching models.

### Batch Size Guidelines

| Model Size | Recommended batch_size |
|-----------|----------------------|
| <300M     | 96-128               |
| 300M-800M | 48-64                |
| 800M-2B   | 24-32                |

Adjust based on available GPU memory.

---

## License Considerations

### Commercial Use

**Allowed:**
- jina-code-0.5b (Apache 2.0)
- Qodo-Embed-1.5B (OpenRAIL++-M)
- jina-v2-base-code (Apache 2.0)

**Not Allowed:**
- jina-code-0.5b (CC-BY-NC-4.0)
- jina-code-1.5b (CC-BY-NC-4.0)

### Non-commercial Use

All models are allowed for:
- Personal projects
- Research
- Academic use
- Open-source projects (depends on license compatibility)

---

## Recommendations by Use Case

### Maximum Search Quality (Non-commercial)
â†’ **jina-code-0.5b** (best balance) or **jina-code-1.5b** (absolute best)

### Commercial Projects
â†’ **Qodo-Embed-1.5B** (only option)

### Resource-Constrained (<8GB RAM)
â†’ **jina-v2-base-code** (161M)

### Multilingual Codebases
â†’ **jina-code-0.5b** (100+ languages)

### Speed-Critical Applications
â†’ **jina-v2-base-code** or **jina-code-0.5b**

### General-Purpose (Code + Docs)
â†’ **jina-code-0.5b** (balanced)

---

## Custom Models

CodeContext supports any HuggingFace embedding model. To use a custom model:

1. Ensure it's compatible with `transformers` library
2. Set `model_name` in `.codecontext.yaml`:
   ```yaml
   embeddings:
     huggingface:
       model_name: "your-org/your-model"
   ```
3. Adjust `batch_size` based on model size
4. Run indexing

**Note:** Embedding dimension is auto-detected from the model.

---

## Further Reading

- [CoIR Benchmark](https://github.com/CoIR-team/coir)
- [HuggingFace Model Hub](https://huggingface.co/models?pipeline_tag=feature-extraction)
- [CodeContext Architecture](../architecture.md)
